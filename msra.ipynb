{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "msra.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1SveTz-CxoL2Dt7xnBSssnhnUzKf6DEth",
      "authorship_tag": "ABX9TyNntqYlaAMJtt9+jRm0Faul",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lokwq/TextBrewer/blob/add_note_examples/msra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kh5ekwgrCG4"
      },
      "source": [
        "This notebook shows how to fine-tune a model in msra_ner datasets and how to distill the model with Textbrewer.\n",
        "\n",
        "Detailed Docs can be find here:\n",
        "https://github.com/airaria/TextBrewer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi_1qOral-LE"
      },
      "source": [
        "import torch\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cahHLZXmI81"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install seqeval\n",
        "!pip install textbrewer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfq_aPKhe9jJ"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer,BertConfig,BertForTokenClassification\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset,load_metric"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvnDEfRewRhJ"
      },
      "source": [
        "Prepare dataset to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtoFe4FPmWh_"
      },
      "source": [
        "task = \"ner\" #  \"ner\", \"pos\" or \"chunk\"\n",
        "model_checkpoint = \"bert-base-chinese\"\n",
        "batch_size = 8"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhvaZdRqmow6"
      },
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "datasets = load_dataset(\"msra_ner\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgM6aADvnIm5",
        "outputId": "500364c0-909a-47ae-c785-686dc2eccb12"
      },
      "source": [
        "label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
        "label_list"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO48GEqlnRuh"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fO7iYQanY1G"
      },
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9iuLfNDoqz6"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9X0l225n7WZ"
      },
      "source": [
        "from transformers import BertForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = BertForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87HPRyytoYb8"
      },
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97wY0abnobYe"
      },
      "source": [
        "metric = load_metric(\"seqeval\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxl_04jup6ep"
      },
      "source": [
        "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENBYA77ToLwV"
      },
      "source": [
        "args = TrainingArguments(\n",
        "    f\"test-{task}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qaj3gN7xovJ0"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzxXm1K1o_Uh"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRiDIS5WuJ7s"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/msra_teacher_model.pt') #save the teacher model weights to distill"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5fK_phaslqc"
      },
      "source": [
        "Start distilling!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z03IrKzzXYMZ"
      },
      "source": [
        "import textbrewer\n",
        "from textbrewer import GeneralDistiller\n",
        "from textbrewer import TrainingConfig, DistillationConfig\n",
        "from transformers import BertForTokenClassification, BertConfig, AdamW,BertTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torch "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YloYaVoWs14s"
      },
      "source": [
        "Initialize the student model by BertConfig and prepare the teacher model.\n",
        "\n",
        "bert_config_L3.json refer to a 3-layer Bert."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P7a1W9NXdH9"
      },
      "source": [
        "bert_config_T3 = BertConfig.from_json_file('/content/drive/MyDrive/TextBrewer-master/examples/student_config/bert_base_cased_config/bert_config_L3.json') \n",
        "bert_config_T3.output_hidden_states = True\n",
        "bert_config_T3.num_labels = len(label_list)\n",
        "\n",
        "student_model = BertForTokenClassification(bert_config_T3)\n",
        "\n",
        "bert_config = BertConfig.from_json_file('/content/drive/MyDrive/TextBrewer-master/examples/student_config/bert_base_cased_config/bert_config.json')\n",
        "bert_config.output_hidden_states = True\n",
        "bert_config.num_labels = len(label_list)\n",
        "\n",
        "teacher_model = BertForTokenClassification(bert_config) \n",
        "teacher_model.load_state_dict(torch.load('/content/drive/MyDrive/msra_teacher_model.pt'))\n",
        "\n",
        "teacher_model.to(device)\n",
        "student_model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKC_jnkuxTJn"
      },
      "source": [
        "The cell below is to distill the teacher model to student model you prepared.\n",
        "\n",
        "After the code execution is complete, the distilled model will be in 'saved_model' in colab file list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6K8OyXuXgsh"
      },
      "source": [
        "num_epochs = 20\n",
        "num_training_steps = len(train_dataloader) * num_epochs\n",
        "\n",
        "optimizer = AdamW(student_model.parameters(), lr=1e-5)\n",
        "\n",
        "scheduler_class = get_linear_schedule_with_warmup\n",
        "\n",
        "scheduler_args = {'num_warmup_steps':int(0.1*num_training_steps), 'num_training_steps':num_training_steps}\n",
        "\n",
        "def simple_adaptor(batch, model_outputs):\n",
        "  return {\"logits\":model_outputs.logits, 'hidden': model_outputs.hidden_states}\n",
        "\n",
        "distill_config = DistillationConfig(\n",
        "    intermediate_matches=[{\"layer_T\":0, \"layer_S\":0, \"feature\":\"hidden\", \"loss\":\"hidden_mse\", \"weight\":1},\n",
        "               {\"layer_T\":4, \"layer_S\":1, \"feature\":\"hidden\", \"loss\":\"hidden_mse\", \"weight\":1},\n",
        "               {\"layer_T\":8, \"layer_S\":2, \"feature\":\"hidden\", \"loss\":\"hidden_mse\", \"weight\":1},\n",
        "               {\"layer_T\":12,\"layer_S\":3, \"feature\":\"hidden\", \"loss\":\"hidden_mse\", \"weight\":1}])\n",
        "\n",
        "train_config = TrainingConfig()\n",
        "distiller = GeneralDistiller(\n",
        "    train_config=train_config, distill_config=distill_config,\n",
        "    model_T=teacher_model, model_S=student_model, \n",
        "    adaptor_T=simple_adaptor, adaptor_S=simple_adaptor)\n",
        "\n",
        "\n",
        "with distiller:\n",
        "    distiller.train(optimizer, train_dataloader, num_epochs, scheduler_class=scheduler_class, scheduler_args = scheduler_args, callback=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmTWyy9dyuqG"
      },
      "source": [
        "Then evaluate the distilled model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTU4sYE-fKN2"
      },
      "source": [
        "bert_config_T3 = BertConfig.from_json_file('/content/drive/MyDrive/data/bert_config/bert_config_L3.json')\n",
        "\n",
        "bert_config_T3.output_hidden_states = True\n",
        "bert_config_T3.num_labels = len(label_list)\n",
        "test_model = BertForTokenClassification(bert_config_T3)\n",
        "\n",
        "\n",
        "test_model.load_state_dict(torch.load('/content/drive/MyDrive/model/gs2813.pkl'))\n",
        "test_model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pc2iuCAZmbh"
      },
      "source": [
        "args = TrainingArguments(\n",
        "    f\"distill-test\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    do_train=False,\n",
        "    do_eval=True,\n",
        "    no_cuda=False,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3VwmhxvZUDR"
      },
      "source": [
        "trainer = Trainer(\n",
        "    test_model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32q6n0fGzjtX"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hq_TkiP1-I6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}